{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from sklearn import metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = os.path.join(\"/DATA\",os.environ.get(\"USER\"),\"MNIST_data\")\n",
    "os.makedirs(local_dir,mode=0o755, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /DATA/solomiia.kurchaba/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /DATA/solomiia.kurchaba/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /DATA/solomiia.kurchaba/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /DATA/solomiia.kurchaba/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(local_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametry sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 28 \n",
    "time_steps = 28 \n",
    "hidden = 128\n",
    "output_classes = 10\n",
    "lr=0.0001\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowanie zbiorów treningowego (x_batches, y_batches) oraz testowego (x_test,y_test)\n",
    "\n",
    "x-dane wejściowe, y-dane wyjściowe (identyfikator obrazka)\n",
    "\n",
    "Dla tego żeby załadować dane wejściowe do sieci muszą być przygotowane w postaci trójwymiarowego tensora, gdzie pierwzy wymiar to wielkość zbioru, drugi-ilość kroków czasowych (ilość kroków z przeszłości, które pamięta sieć).Trzeci wymiar - ilosć zmiennych ładowanych do sieci w jednym kroku czasowym. Ponieważ mamy obrazki 28x28, przedstawiamy ich jako 28 zmiennych ładowanych w 28-miu krokach czasowych.\n",
    "\n",
    "Dane wyjściowe zostawiamy tak jak są, czyli dwuwymiarowy tensor, gdzie pierwszy wymiar wielkość zbioru, drugi-liczba zmiennych.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_batches = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "x_batches = x_tr.reshape(-1, time_steps, num_input)\n",
    "x_t=mnist.test.images\n",
    "x_test=x_t.reshape(-1,time_steps,num_input)\n",
    "y_test=mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 784) (128, 28, 28) (128, 10)\n",
      "(10000, 784) (10000, 28, 28)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_b),np.shape(x_batches),np.shape(y_batches))\n",
    "print(np.shape(x_t),np.shape(x_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konstrukcja sieci\n",
    "\n",
    "1. Rozmiary Placecholderów dla danych wejściowych i wyjściowych muszą być zgodne z wymiarem przygotowanych danych;\n",
    "2. Ponieważ korzystamy ze statycznej wersji sieci musimy zgóry podać ile kroków czasowych zamierzamy robić. Dlatego dane wejściowe muszą być jeszcze raz transformowane.\n",
    "3. Po funkcji unstack mamy 28 tensorów (bo mamy 28 kroków czasowych) o wymiarze (rozmiar zbioru, ilość zmiennych)\n",
    "4. Zamiast LSTM korzystam z RNN, tak jak ma taką samą zasadę działania, ale posiada dużo mniej parametrów wewnętrznych-proces uczenia się przebiega szybciej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,[None,time_steps,num_input])\n",
    "Y=tf.placeholder(tf.float32,[None,output_classes])\n",
    "x=tf.unstack(X,time_steps,1)\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden) # here the first set of weights and biases is defined\n",
    "rnn_output, states=rnn.static_rnn(basic_cell,x,dtype=tf.float32)\n",
    "stacked_rnn_output=tf.reshape(rnn_output[-1],[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output_classes)# the output layer. (+additional set of weights and biases)\n",
    "predict=tf.nn.softmax(stacked_outputs)\n",
    "y_pred=tf.argmax(predict,1)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=Y))\n",
    "\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    "training_op=optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(predict, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init=tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktywacja sieci\n",
    "Ładujemy dane treningowe i testowe. Każde 100 iteracji treningowe wyświetlana wartość entropii.\n",
    "\n",
    "Procent uzyskanych dobrych odpowiedzi podczas ostatniej iteracji traningowej: 99.22%\n",
    "\n",
    "Procent uzyskanych poprawnych odpowiedzi na zbiorze testowym: 61.82%\n",
    "\n",
    "Widzimy że rekurencyjne sieć na takiego typu danych działa dość kiepsko.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tloss: 2.29335\n",
      "100 \tloss: 1.90679\n",
      "200 \tloss: 1.662\n",
      "300 \tloss: 1.55047\n",
      "400 \tloss: 1.5141\n",
      "500 \tloss: 1.4886\n",
      "600 \tloss: 1.4842\n",
      "700 \tloss: 1.48219\n",
      "800 \tloss: 1.4809\n",
      "900 \tloss: 1.47369\n",
      "0.992188\n",
      "0.6182\n"
     ]
    }
   ],
   "source": [
    "epochs=1000\n",
    "with  tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        loss=loss_op.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            loss=loss_op.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tloss:\",loss)\n",
    "    train_accuracy=accuracy.eval(feed_dict={X: x_batches, Y: y_batches})  \n",
    "    print(train_accuracy)\n",
    "    test_accuracy=accuracy.eval(feed_dict={X:x_test, Y:y_test}) \n",
    "    print(test_accuracy)\n",
    "        \n",
    "        \n",
    "sess.close()        \n",
    "            \n",
    "  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
